{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPlYq6aBZScOOUeVsltHvOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbwiersum/gbwiersum/blob/main/RagBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Xf5kQC_ak9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "8c8aaf5b-581b-4359-bfe8-e71932a4346e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_ollama'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-222a6c0f6abd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_ollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOllamaEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_ollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_text_splitters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_ollama'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "import streamlit as st\n",
        "import ollama\n",
        "\n",
        "base_url = \"localhost:11434/\"\n",
        "modlist = [m[\"model\"] for m in ollama.list()[\"models\"]]\n",
        "modelname = modlist[0]\n",
        "response={\"answer\":\"\"}\n",
        "\n",
        "def start_llm(modelname, base_url='localhost:11434', temperature=0.2, num_predict=512, top_k=50, top_p=50):\n",
        "    llm = ChatOllama(model=modelname,\n",
        "                    base_url=base_url,\n",
        "                    temperature=temperature,\n",
        "                    num_predict=num_predict,\n",
        "                    top_k=top_k,\n",
        "                    top_p=top_p\n",
        "                        )\n",
        "    return llm\n",
        "\n",
        "sysprompt = \"\"\"You are a generative AI assistant designed to help\n",
        "researchers working in national defense.\n",
        "Your answers must work step-by-step to build an analysis synthesizing\n",
        "from the information provided. Above all, you're constructing an argument.\n",
        "The goal is to return the logical steps and information sources used\n",
        "to answer the question. If you don't have sufficient information reply\n",
        "\"I don't know\".\"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate([(\"system\", sysprompt),\n",
        "                            (\"user\", \"{question}\"),\n",
        "                            (\"system\", \"{context}\"),\n",
        "                            \"assistant:\"])\n",
        "\n",
        "embeddings = OllamaEmbeddings(model = modelname, base_url=base_url)\n",
        "vector_store = InMemoryVectorStore.from_documents(\"\", embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loaders for working with BytesIO that Streamlit generates:\n",
        "# These should go into RagBot definitions or maybe a utils?\n",
        "from io import BytesIO\n",
        "from langchain_community.document_loaders.parsers.pdf import PyPDFParser\n",
        "from langchain_core.document_loaders.base import BaseLoader\n",
        "from langchain_core.document_loaders.blob_loaders import Blob\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class RagBot:\n",
        "    def __init__(self, llm, vector_store):\n",
        "        self.llm = llm\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    class CustomPDFLoader(BaseLoader):\n",
        "        def __init__(self, stream: BytesIO, password =None, extract_images: bool = False):\n",
        "            self.stream = stream\n",
        "            self.parser = PyPDFParser(password=password, extract_images=extract_images)\n",
        "\n",
        "        def load(self) -> list[Document]:\n",
        "            blob = Blob.from_data(self.stream.getvalue())\n",
        "            return list(self.parser.parse(blob))\n",
        "\n",
        "    def add_vector(pages:list[Document], chunk_size:int =1000, chunk_overlap: int =200, vector_store=vector_store):\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "        all_splits = text_splitter.split_documents(pages)\n",
        "        vector_store.add_documents(all_splits)\n",
        "\n",
        "    def pdf_from_path(self, filepath):\n",
        "        loader = PyPDFLoader(filepath)\n",
        "        pages = [p for p in loader.lazy_load()]\n",
        "        self.add_vector(pages)\n",
        "\n",
        "    def pdf_from_bytes(self, uploaded: list[list, bytes]) -> Document:\n",
        "        if type(uploaded)==list:\n",
        "            for u in uploaded:\n",
        "                cpl = self.CustomPDFLoader(u)\n",
        "                pages = [p for p in cpl.lazy_load()]\n",
        "                self.add_vector(pages)\n",
        "        else:\n",
        "            cpl = self.CustomPDFLoader(uploaded)\n",
        "            pages = [p for p in cpl.lazy_load()]\n",
        "            self.add_vector(pages)\n",
        "\n",
        "\n",
        "    # Define state for application\n",
        "    class State(TypedDict):\n",
        "        question: str\n",
        "        context: List[Document]\n",
        "        answer: str\n",
        "\n",
        "    # Define application steps\n",
        "    def retrieve(state: State):\n",
        "        retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "        return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "    def generate(self, state: State):\n",
        "        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "        messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "        response = self.llm.invoke(messages)\n",
        "        return {\"answer\": response.content}\n",
        "\n",
        "    def run_query(self, question: str):\n",
        "        retrieved_docs = vector_store.similarity_search(question)\n",
        "        got = {\"context\": retrieved_docs}\n",
        "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "        messages = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
        "        response = self.llm.invoke(messages)\n",
        "        return response, got"
      ],
      "metadata": {
        "id": "OebYHZX_ltjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = start_llm(modlist[0], temperature=0.8, num_predict=1028, top_k=100, top_p=100)\n",
        "Bot = RagBot(llm, vector_store)\n",
        "\n",
        "def add_folder(folder):\n",
        "    for f in [p for p in os.walk(folder)][0][2]:\n",
        "        print(\"adding: {f}\")\n",
        "        pdf_from_path(folder+\"/\"+f)\n",
        "\n",
        "add_folder(\"/mnt/c/Users/gwiersum/Reference\")"
      ],
      "metadata": {
        "id": "zbwl-X-elxa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex = Bot.run_query(\"What is the meaning of life, the universe and everything?\")\n",
        "print(ex[0].content)\n",
        "print([i.metadata for i in ex[1][\"context\"]])"
      ],
      "metadata": {
        "id": "TIdPTUQHl9kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8u_dImP6l4Uf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}